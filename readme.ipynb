{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd95bec1-cee9-421d-b3bc-b646264e6e4b",
   "metadata": {},
   "source": [
    "# TP53_SGE\n",
    "\n",
    "\n",
    "\n",
    "## Instructions for use\n",
    "\n",
    "\n",
    "### Generating counts from fastqs\n",
    "\n",
    "The generation of mutant sequence counts from fastqs obtained via targeted sequencing\n",
    "involved several steps.\n",
    "\n",
    "\n",
    "#### Strategy to obtain sequence counts from multiplexed paired-end libraries\n",
    "\n",
    "1. Demultiplexing the libraries\n",
    "\n",
    "   Sequenced libraries typically contained multiplexed samples or replicates.\n",
    "   Based on a set of sample-specific barcodes that comprised the fist n nucleotides\n",
    "   of the R1 and R2 read pairs, the initial step involved demultiplexing the libraries\n",
    "   and trimming the adapter barcodes.\n",
    "   \n",
    "2. Merging paired-end reads\n",
    "\n",
    "   After demultiplexing, paired-end reads were merged into a single sequence.\n",
    "   Since the targeted sequencing resulted in overlapping paired-end reads, we\n",
    "   used this fact to eliminate read pairs that showed divergence in the overlapping\n",
    "   region. As a result, these pairs could not be merged and were discarded.\n",
    "\n",
    "3. Counting the sequences\n",
    "\n",
    "   As our mutant library contained every possible SNV, the merged sequences would potantially\n",
    "   contain sequences that would differ by only a single nucleotide. Since this precluded\n",
    "   error-tolerant alignment strategies, counting sequences is reduced to a simple\n",
    "   exact-matching problem. Moreover, since our mutant library sequences all contained constant flanking regions,\n",
    "   we trimmed the merged reads so that only the mutatable sequence remained and then simply counted the occurence\n",
    "   of each synthetic sequence directly.\n",
    "\n",
    "\n",
    "#### The run-script\n",
    "\n",
    "   The [main.py](main.py) python script contains the complete run script used to create these counts.\n",
    "   \n",
    "   It uses pypipegraph2 as a job-scheduling framework in combination with a python environment \n",
    "   on a NixOS based server infrastructure.\n",
    "\n",
    "\n",
    "### Obtaining Relative Fitness Scores (RFS)\n",
    "\n",
    "The different lengths of the targeted exons resulted in a bias that precluded a direct\n",
    "comparison of the raw counts or the relative frequencies of mutant sequences between the\n",
    "exons. In order to obtain a normalized and comparable score across all exons, performed the following normalization:\n",
    "\n",
    "To calculate the relative frequencies (variant abundances), the read count was divided by the total \n",
    "number of matched reads. From this ratio, we obtained Enrichment scores ($ES$) as the negative log2 fold change \n",
    "of the variant abundance in treated versus control conditions. \n",
    "\n",
    "However, this $ES$ is dependent on the relative amount of wild-type-like and loss-of-function variants in a cell population, which varies between different libraries. To obtain a score that is comparable across different libraries and\n",
    "screens, the $ES$ was further nromalized into a relative fitness score ($RFS$) by the follwing formula:\n",
    "\n",
    "$RFS_{ex}(ES) = (\\frac{ES - \\tilde x_{ex}^{non}}{\\tilde x_{ex}^{non} - \\tilde x_{ex}^{syn}}) * 2 + 1$\n",
    "\n",
    "with $\\tilde x_{ex}^{non}$ denoting the median of the scores\n",
    "for all nonsense mutations in a specific exon $ex$ and $\\tilde x_{ex}^{syn}$ denoting the median of all synonymous mutations\n",
    "in this exon.\n",
    "\n",
    "$RFS$ scores were calculated for each replicate, then, as our total score, we obtained the median ($RFS_{median}$) over\n",
    "all replicates. \n",
    "\n",
    "\n",
    "The calculation can be found in the following jupyter notebook:\n",
    "\n",
    "[RFS.ipynb](RFS.ipynb)\n",
    "   \n",
    "The same transformation was performed on the Enrich2 scores, the corresponding calculation can be found in:\n",
    "\n",
    "[RFS_enrich.ipynb](RFS_enrich.ipynb)\n",
    "\n",
    "### Performing Enrich2 analyses\n",
    "\n",
    "The Enrich2 package was used to perform an additional analysis on our variant counts.\n",
    "More precisely, we used Enrich2 in count-mode by supplying our variant counts as \n",
    "\"Identifiers Only\" SeqLib. These can be found in the folder \"enrich2_files\".\n",
    "\n",
    "The analysis was run via the script [run_enrich.py](run_enrich.py), calling the command line option enrich2_cmd in a\n",
    "dedicated conda environment as instructed.\n",
    "\n",
    "For our analysis, we used the configuration file [Exon5678_identifier](enrich2_files/Exon5678_identifier), which is also found in the \"enrich2_files\" folder.\n",
    "\n",
    "We used Enrich2 log ratios as scoring method, with DMSO treated samples as T0 and N3a treated samples as T1.\n",
    "As normalization method we used the library size (full) option.\n",
    "\n",
    "\n",
    "### Calculating p-values\n",
    "\n",
    "We used the RFS-transformed scores and standard errors of the enrich2 analysis as calculated above (in [RFS_enrich.ipynb](RFS_enrich.ipynb)) to perform one-sided z-tests for each non-synonymous variant under the null hypothesis that the variantâ€™s score is equal or lower from the weighted mean of the synonymous (WT-like) variants. The alternative hypothesis being that the scores obtained from non-synonymous variants are significantly higher. The z-tests are then adjusted for multiple hypothesis testing using Benjamini-Hochberg correction.\n",
    "\n",
    "The calculation can be found in this jupyter notebook: [z_statistics_enrich.ipynb](z_statistics_enrich.ipynb)\n",
    "\n",
    "### How to use it \n",
    "\n",
    "The listed python scripts should be run in a python environment with the \n",
    "installed dependencies listed below.\n",
    "Adjust the variables in the section marked as \"initialize variables\"\n",
    "to the specifics of your folder structure, if needed.\n",
    "\n",
    "Jupyter notebooks can be run as-is inside any python environment with pandas \n",
    "(version >= 2.0) and jupyter installed.\n",
    "\n",
    "\n",
    "### Demo\n",
    "\n",
    "the file [demo.py](demo.py) contains a demo on how to run the counting analysis.\n",
    "\n",
    "It can be run in a python environment with the appropriate dependencies installed by calling\n",
    "\n",
    "```bash\n",
    "python demo.py\n",
    "```\n",
    "\n",
    "on the command line.\n",
    "\n",
    "The neccessary input files are deposited in the demo folder:\n",
    "\n",
    "[NovaSeq_incoming_sequences_Exon5demo.xlsx](demo/NovaSeq_incoming_sequences_Exon5demo.xlsx) contains\n",
    "an excel sheet with example barcodes akin to those used in our analysis for the demultiplexing step.\n",
    "\n",
    "Another excel sheet in the same file contains a couple of predefined sequences that should be found in\n",
    "the incoming fastq files.\n",
    "\n",
    "Expected output is a folder with counted sequences and a summary table, that\n",
    "will list each of the Exon 5 input sequences with a count of 1.\n",
    "\n",
    "Execution time should be 10 seconds on an average computer.\n",
    "\n",
    "## System requirements\n",
    "\n",
    "### Python\n",
    "\n",
    "All source code was run on python, version >= 3.8.\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "The analysis depends on the following python packages that were installed on our python environment:\n",
    "\n",
    "- pandas (version = 2.1)\n",
    "- numpy (version = 1.26)\n",
    "- mbf: https://github.com/imTMarburg/mbf (version = 0.1)\n",
    "- pypipegraph2: https://github.com/TyberiusPrime/pypipegraph2/ (version = 3.0)\n",
    "- mmdemultiplex: https://github.com/MarcoMernberger/mmdemultiplex (version = 0.5.0)\n",
    "- counting_sequences: https://github.com/MarcoMernberger/counting_sequences (version = 0.5)\n",
    "- NGMerge: https://github.com/jsh58/NGmerge (version = 0.1)\n",
    "- cutadapt: https://github.com/marcelm/cutadapt (version = 4.1)\n",
    "\n",
    "The version numbers indicate the versions our scripts have been tested on.\n",
    "\n",
    "In addition, the packages above may have further dependencies.\n",
    "\n",
    "### Operating System\n",
    "\n",
    "The analyses were run on NixOS servers with the anysnake2 package manager. \n",
    "\n",
    "Jupyter notebooks were created and run on jupyter-server version >= 1.23.\n",
    "\n",
    "\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install pandas, numpy, mbf and pypipegraph2 from pypi via:\n",
    "\n",
    "```bash\n",
    "pip install <package>\n",
    "```\n",
    "\n",
    "The other packages can be ontained via github using:\n",
    "\n",
    "```bash\n",
    "git clone <url>\n",
    "```\n",
    "\n",
    "For cutadapt and NGmerge follow package instructions on how to build them.\n",
    "\n",
    "Typical install time on an everage computer should be less than 5 minutes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
