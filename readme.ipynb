{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd95bec1-cee9-421d-b3bc-b646264e6e4b",
   "metadata": {},
   "source": [
    "\n",
    "## Generating counts from fastqs\n",
    "\n",
    "The generation of mutant sequence counts from fastqs obtained via targeted sequencing\n",
    "involved several steps.\n",
    "\n",
    "\n",
    "#### Strategy to obtain sequence counts from multiplexed paired-end libraries\n",
    "\n",
    "1. Demultiplexing the libraries\n",
    "\n",
    "   Sequenced libraries typically contained multiplexed samples or replicates.\n",
    "   Based on a set of sample-specific barcodes that comprised the fist n nucleotides\n",
    "   of the R1 and R2 read pairs, the initial step involved demultiplexing the libraries\n",
    "   and trimming the adapter barcodes.\n",
    "   \n",
    "2. Merging paired-end reads\n",
    "\n",
    "   After demultiplexing, paired-end reads were merged into a single sequence.\n",
    "   Since the targeted sequencing resulted in overlapping paired-end reads, we\n",
    "   used this fact to eliminate read pairs that showed divergence in the overlapping\n",
    "   region. As a result, these pairs could not be merged and were discarded.\n",
    "\n",
    "3. Counting the sequences\n",
    "\n",
    "   As our mutant library contained every possible SNV, the merged sequences would potantially\n",
    "   contain sequences that would differ by only a single nucleotide. Since this precluded\n",
    "   error-tolerant alignment strategies, counting sequences is reduced to a simple\n",
    "   exact-matching problem. Moreover, since our mutant library sequences all contained constant flanking regions,\n",
    "   we trimmed the merged reads so that only the mutatable sequence remained and then simply counted the occurence\n",
    "   of each synthetic sequence directly.\n",
    "\n",
    "\n",
    "#### The run-script\n",
    "\n",
    "   The [main.py](main.py) python script contains the complete run script used to create these counts.\n",
    "   \n",
    "   It uses pypipegraph2 as a job-scheduling framework in combination with a python environment \n",
    "   on a NixOS based server infrastructure.\n",
    "\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "The analysis depends on the following python packages that were installed on our python environment:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- NGMerge: https://github.com/jsh58/NGmerge\n",
    "- cutadapt: https://github.com/marcelm/cutadapt\n",
    "- mbf: https://github.com/imTMarburg/mbf\n",
    "- pypipegraph2: https://github.com/TyberiusPrime/pypipegraph2/\n",
    "- mmdemultiplex: https://github.com/MarcoMernberger/mmdemultiplex\n",
    "- counting_sequences: https://github.com/MarcoMernberger/counting_sequences\n",
    "\n",
    "\n",
    "## Obtaining Relative Fitness Scores (RFS)\n",
    "\n",
    "The different lengths of the targeted exons resulted in a bias that precluded a direct\n",
    "comparison of the raw counts or the relative frequencies of mutant sequences between the\n",
    "exons. In order to obtain a normalized and comparable score across all exons, performed the following normalization:\n",
    "\n",
    "To calculate the relative frequencies (variant abundances), the read count was divided by the total \n",
    "number of matched reads. From this ratio, we obtained Enrichment scores ($ES$) as the negative log2 fold change \n",
    "of the variant abundance in treated versus control conditions. \n",
    "\n",
    "However, this $ES$ is dependent on the relative amount of wild-type-like and loss-of-function variants in a cell population, which varies between different libraries. To obtain a score that is comparable across different libraries and\n",
    "screens, the $ES$ was further nromalized into a relative fitness score ($RFS$) by the follwing formula:\n",
    "\n",
    "$RFS_{ex}(ES) = (\\frac{ES - \\tilde x_{ex}^{non}}{\\tilde x_{ex}^{non} - \\tilde x_{ex}^{syn}}) * 2 + 1$\n",
    "\n",
    "with $\\tilde x_{ex}^{non}$ denoting the median of the scores\n",
    "for all nonsense mutations in a specific exon $ex$ and $\\tilde x_{ex}^{syn}$ denoting the median of all synonymous mutations\n",
    "in this exon.\n",
    "\n",
    "$RFS$ scores were calculated for each replicate, then, as our total score, we obtained the median ($RFS_{median}$) over\n",
    "all replicates. \n",
    "\n",
    "\n",
    "The calculation can be found in the following jupyter notebook:\n",
    "\n",
    "[RFS.ipynb](RFS.ipynb)\n",
    "   \n",
    "The same transformation was performed on the Enrich2 scores, the corresponding calculation can be found in:\n",
    "\n",
    "[RFS_enrich.ipynb](RFS_enrich.ipynb)\n",
    "\n",
    "## Performing Enrich2 analyses\n",
    "\n",
    "The Enrich2 package was used to perform an additional analysis on our variant counts.\n",
    "More precisely, we used Enrich2 in count-mode by supplying our variant counts as \n",
    "\"Identifiers Only\" SeqLib. These can be found in the folder.\n",
    "\n",
    "The analysis was run via the script [run_enrich.py](run_enrich.py), calling the command line option enrich2_cmd in a\n",
    "dedicated conda environment as instructed.\n",
    "\n",
    "For our analysis, we used the configuration file [Exon5678_identifier](enrich2_files/Exon5678_identifier), which is also found in the enrich2_files folder.\n",
    "\n",
    "We used Enrich2 log ratios as scoring method, with DMSO treated samples as T0 and N3a treated samples as T1.\n",
    "As log ratio method we used the library size (full) option.\n",
    "\n",
    "\n",
    "## Calculating p-values\n",
    "\n",
    "We used the RFS-transformed scores and standard errors of the enrich2 analysis as calculated above (in [RFS_enrich.ipynb](RFS_enrich.ipynb)) to perform one-sided z-tests for each non-synonymous variant under the null hypothesis that the variantâ€™s score is equal or lower from the weighted mean of the synonymous (WT-like) variants. The alternative hypothesis being that the scores obtained from non-synonymous variants are significantly higher. The z-tests are then adjusted for multiple hypothesis testing using Benjamini-Hochberg correction.\n",
    "\n",
    "The calculation can be found in this jupyter notebook: [z_statistics_enrich.ipynb](z_statistics_enrich.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
